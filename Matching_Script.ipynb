{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fe983d-3ea5-4ec5-ab75-943f4bb254c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching entities: 100%|██████████| 592/592 [00:00<00:00, 1011.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching completed.\n",
      "Results saved to 'presales_match_results.csv'\n",
      "Summary:\n",
      "Total inputs: 592\n",
      "Matched: 525\n",
      "Needs Review: 47\n",
      "Unmatched: 20\n",
      "% Matched: 88.7%\n",
      "Duplicate veridion_id count: 7\n",
      "Details of duplicates saved to 'presales_duplicate_veridion_ids.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Presales Entity Matching\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "INPUT_FILE = \"presales_data_sample.csv\"  # Path to the presales sample CSV\n",
    "OUTPUT_FILE = \"presales_match_results.csv\"  # Where to save the matching results\n",
    "\n",
    "# Thresholds\n",
    "THRESHOLD_MATCH = 85       # Score >= this is Matched\n",
    "THRESHOLD_REVIEW = 75      # Score >= this and < MATCH is Needs Review\n",
    "\n",
    "# --- Normalize company names ---\n",
    "def clean_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a company name by lowercasing, removing diacritics,\n",
    "    stripping legal suffixes and punctuation, and collapsing whitespace.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    s = unidecode(str(name)).lower().strip()\n",
    "    legal_suffixes = [\n",
    "        \"inc\", \"inc.\", \"llc\", \"ltd\", \"gmbh\", \"ag\", \"srl\",\n",
    "        \"sa\", \"bv\", \"plc\", \"co\", \"company\", \"corp\",\n",
    "        \"corporation\", \"aps\", \"a/s\"\n",
    "    ]\n",
    "    for suffix in legal_suffixes:\n",
    "        if s.endswith(f\" {suffix}\"):\n",
    "            s = s[: -len(suffix) - 1]\n",
    "    for ch in ['-', ',', '.', '/', '\"', \"'\", \"(\", \")\"]:\n",
    "        s = s.replace(ch, ' ')\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "# --- Compute weighted fuzzy match score between input and candidate rows ---\n",
    "def compute_match_score(input_row: pd.Series, candidate_row: pd.Series) -> float:\n",
    "    name_input = clean_name(input_row['input_company_name'])\n",
    "    candidate_name = clean_name(candidate_row['company_name'])\n",
    "    candidate_legal = clean_name(candidate_row.get('company_legal_names', ''))\n",
    "\n",
    "    # Fuzzy similarity on name vs legal\n",
    "    score_name = fuzz.token_sort_ratio(name_input, candidate_name)\n",
    "    score_legal = fuzz.token_sort_ratio(name_input, candidate_legal)\n",
    "    best_name_score = max(score_name, score_legal)\n",
    "\n",
    "    # Country and region bonuses\n",
    "    bonus = 0.0\n",
    "    if 'input_main_country' in input_row and 'main_country' in candidate_row:\n",
    "        if str(input_row['input_main_country']).strip().lower() == str(candidate_row['main_country']).strip().lower():\n",
    "            bonus += 0.25  # +25%\n",
    "    if 'input_main_region' in input_row and 'main_region' in candidate_row:\n",
    "        if str(input_row['input_main_region']).strip().lower() == str(candidate_row['main_region']).strip().lower():\n",
    "            bonus += 0.15  # +15%\n",
    "\n",
    "    final_score = best_name_score + bonus * 100\n",
    "    return min(final_score, 100)\n",
    "\n",
    "# --- Core matching function (returns detailed rows) ---\n",
    "def presales_entity_matching(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    for input_key, group in tqdm(df.groupby('input_row_key'), desc=\"Matching entities\"):\n",
    "        input_row = group.iloc[0]\n",
    "        best_score = -1.0\n",
    "        best_candidate = None\n",
    "\n",
    "        # Evaluate each candidate\n",
    "        for _, candidate_row in group.iterrows():\n",
    "            score = compute_match_score(input_row, candidate_row)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_candidate = candidate_row\n",
    "\n",
    "        # Determine match status\n",
    "        if best_score >= THRESHOLD_MATCH:\n",
    "            status = 'Matched'\n",
    "            review = False\n",
    "        elif best_score >= THRESHOLD_REVIEW:\n",
    "            status = 'Needs Review'\n",
    "            review = True\n",
    "        else:\n",
    "            status = 'Unmatched'\n",
    "            review = False\n",
    "\n",
    "        # Append one detailed result per input_key\n",
    "        results.append({\n",
    "            # Input fields\n",
    "            'input_row_key': input_key,\n",
    "            'input_company_name': input_row['input_company_name'],\n",
    "            'input_main_country_code': input_row.get('input_main_country_code', None),\n",
    "            'input_main_country': input_row.get('input_main_country', None),\n",
    "            'input_main_region': input_row.get('input_main_region', None),\n",
    "            'input_main_city': input_row.get('input_main_city', None),\n",
    "            # Candidate fields\n",
    "            'matched_veridion_id': best_candidate['veridion_id'] if best_candidate is not None else np.nan,\n",
    "            'matched_company_name': best_candidate['company_name'] if best_candidate is not None else np.nan,\n",
    "            'company_legal_names': best_candidate.get('company_legal_names', None) if best_candidate is not None else None,\n",
    "            'main_country_code': best_candidate.get('main_country_code', None) if best_candidate is not None else None,\n",
    "            'main_country': best_candidate.get('main_country', None) if best_candidate is not None else None,\n",
    "            'main_region': best_candidate.get('main_region', None) if best_candidate is not None else None,\n",
    "            'main_city': best_candidate.get('main_city', None) if best_candidate is not None else None,\n",
    "            'main_postcode': best_candidate.get('main_postcode', None) if best_candidate is not None else None,\n",
    "            'main_street': best_candidate.get('main_street', None) if best_candidate is not None else None,\n",
    "            'main_street_number': best_candidate.get('main_street_number', None) if best_candidate is not None else None,\n",
    "            'linkedin_url': best_candidate.get('linkedin_url', None) if best_candidate is not None else None,\n",
    "            'website_url': best_candidate.get('website_url', None) if best_candidate is not None else None,\n",
    "            # Meta\n",
    "            'match_score': round(best_score, 1),\n",
    "            'match_status': status,\n",
    "            'review_needed': review\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Script entry point ===\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    results_df = presales_entity_matching(df)\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    # Summary\n",
    "    total = len(results_df)\n",
    "    matched = (results_df['match_status'] == 'Matched').sum()\n",
    "    needs_review = (results_df['match_status'] == 'Needs Review').sum()\n",
    "    unmatched = (results_df['match_status'] == 'Unmatched').sum()\n",
    "\n",
    "    print(\"✅ Matching completed.\")\n",
    "    print(f\"Results saved to '{OUTPUT_FILE}'\")\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Total inputs: {total}\")\n",
    "    print(f\"Matched: {matched}\")\n",
    "    print(f\"Needs Review: {needs_review}\")\n",
    "    print(f\"Unmatched: {unmatched}\")\n",
    "    print(f\"% Matched: {matched/total*100:.1f}%\")\n",
    "\n",
    "    # --- Duplicate veridion_id check ---\n",
    "    dupes = results_df[results_df.duplicated(subset=['matched_veridion_id'], keep=False) & results_df['matched_veridion_id'].notna()]\n",
    "    dup_count = dupes['matched_veridion_id'].nunique()\n",
    "    print(f\"Duplicate veridion_id count: {dup_count}\")\n",
    "    if dup_count > 0:\n",
    "        dupes.to_csv(\"presales_duplicate_veridion_ids.csv\", index=False)\n",
    "        print(\"Details of duplicates saved to 'presales_duplicate_veridion_ids.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bbbae-919e-4658-8c99-d5b57ca4d1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
